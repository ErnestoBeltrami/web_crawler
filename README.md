ðŸš€ Features

âœ… Crawl web pages recursively using a queue
âœ… Extract text and links with BeautifulSoup
âœ… Build an inverted index (word â†’ pages containing that word)
âœ… Simple search engine with ranking by frequency
âœ… Configurable max crawl depth and headers
âœ… Modular structure with three main classes:

UrlQueue â†’ manages URLs and crawling loop
Crawler â†’ parses HTML and extracts content
Indexer â†’ builds and queries the inverted index

ðŸ§© Project Structure
web_crawler/
â”‚
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ url_queue.py     # orchestrates crawling
â”‚   â”œâ”€â”€ crawl.py         # HTML parsing and link 
â”‚   â”œâ”€â”€ indexer.         # inverted index logic
â”œâ”€â”€ main.py              # entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


ðŸ§® How It Works
UrlQueue manages the queue of URLs to visit and limits crawl depth.
Crawler fetches and parses each page using requests and BeautifulSoup.
Indexer stores text in an inverted index for fast lookups.
The user can query words or phrases to see which pages contain them.